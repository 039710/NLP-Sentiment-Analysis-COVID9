{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import imblearn\n",
    "import sklearn.metrics as metrics\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from numpy import where\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening Dataset and convert it into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets :  1211\n"
     ]
    }
   ],
   "source": [
    "dataset_training = []\n",
    "with open ('../dataset/dataset_training.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=';')\n",
    "    \n",
    "    for row in readCSV:\n",
    "        tweets = row['tweet']    \n",
    "        usernames = row['username']\n",
    "        hashtags = row['hashtag']\n",
    "        dates = row['date']\n",
    "        intent = row['intent']\n",
    "        emotion = row['emotion']\n",
    "        dataset_training.append({\n",
    "            'date' : dates,\n",
    "            'username' : usernames,\n",
    "            'tweet' : tweets,\n",
    "            'hashtag' : hashtags,\n",
    "            'intent' : intent,\n",
    "            'emotion' : emotion\n",
    "        })\n",
    "            \n",
    "        \n",
    "print(\"total tweets : \" , len(dataset_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets :  522\n"
     ]
    }
   ],
   "source": [
    "dataset_testing = []\n",
    "with open ('../dataset/dataset_testing.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=';')\n",
    "    \n",
    "    for row in readCSV:\n",
    "        tweets = row['tweet']    \n",
    "        usernames = row['username']\n",
    "        hashtags = row['hashtag']\n",
    "        dates = row['date']\n",
    "        intent = row['intent']\n",
    "        emotion = row['emotion']\n",
    "        dataset_testing.append({\n",
    "            'date' : dates,\n",
    "            'username' : usernames,\n",
    "            'tweet' : tweets,\n",
    "            'hashtag' : hashtags,\n",
    "            'intent' : intent,\n",
    "            'emotion' : emotion\n",
    "        })\n",
    "            \n",
    "        \n",
    "print(\"total tweets : \" , len(dataset_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of normalization are to remove non Indonesian word such as slang words and abbreviations will be converted to standard Indonesian words. the list is created by iterating to each feature in the dataset and list them with their normal word (standar word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of typo words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total typo words :  169\n"
     ]
    }
   ],
   "source": [
    "typo_list = []\n",
    "with open ('../feature_list/typo.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['typo']    \n",
    "        standard_word = row['standard_form']\n",
    "        typo_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })  \n",
    "print(\"total typo words : \" , len(typo_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of abbreviations words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total abbreviation words :  362\n"
     ]
    }
   ],
   "source": [
    "abbreviation_list = []\n",
    "with open ('../feature_list/abbreviation.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['abbreviations']    \n",
    "        standard_word = row['non-abbreviations']\n",
    "        abbreviation_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })      \n",
    "print(\"total abbreviation words : \" , len(abbreviation_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total slang words :  58\n"
     ]
    }
   ],
   "source": [
    "slang_list = []\n",
    "with open ('../feature_list/slangwords.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['slang_word']    \n",
    "        standard_word = row['formal_word']\n",
    "        slang_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })     \n",
    "print(\"total slang words : \" , len(slang_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load combination list of slang words,abbreviation words, and typo words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word_list :  697\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "with open ('../feature_list/word_list.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['word']    \n",
    "        standard_word = row['formal_word']\n",
    "        word_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })    \n",
    "print(\"total word_list : \" , len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def languagePreprocessing(dataset):\n",
    "    for row in dataset:\n",
    "        word_tokens = word_tokenize(row['tweet']) \n",
    "        filtered_word = []\n",
    "        for x in range(len(word_tokens)):\n",
    "            for n in range(len(word_list)):\n",
    "                if word_tokens[x] == word_list[n]['word']:\n",
    "                    word_tokens[x] = word_list[n]['standard_word']\n",
    "                    filtered_word.append(word_tokens[x])\n",
    "            filtered_word.append(word_tokens[x])\n",
    "            filtered_word = list(dict.fromkeys(filtered_word))\n",
    "        row['tweet']= \" \".join(filtered_word)\n",
    "        \n",
    "languagePreprocessing(dataset_training)\n",
    "languagePreprocessing(dataset_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After each list for each category listed, then I manually combine them into one csv file with two columns which are \"word\" and standard word\".\n",
    "By iterating each words after tokenization, if the word match in the document, then the word will be changed based on standard word in csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming, Tokenization, Stopwords removal, remove unnecessary attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load indonesian stopwords list (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword\n",
    "stopWords = []\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('atuser')\n",
    "    stopWords.append('url')\n",
    "    stopWords.append('via')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "st = open('../feature_list/stopwordsID.txt', 'r')\n",
    "stopWords = getStopWordList('../feature_list/stopwordsID.txt')\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation word detected :  tidak => menang\n",
      "negation word detected :  tidak => manfaat\n",
      "negation word detected :  tidak => nya\n",
      "negation word detected :  belum => sampe\n",
      "negation word detected :  tidak => harga\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tidak => bertanggungjawab\n",
      "negation word detected :  tidak => jangkau\n",
      "negation word detected :  tidak => pake\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tak => sajaindonesiaterserah\n",
      "negation word detected :  tidak => tetangga\n",
      "negation word detected :  tidak => kerumun\n",
      "negation word detected :  tak => luput\n",
      "negation word detected :  tak => istilah\n",
      "negation word detected :  kurang => sana\n",
      "negation word detected :  tidak => full\n",
      "negation word detected :  kurang => polsekkubu\n",
      "negation word detected :  tidak => layak\n",
      "negation word detected :  kurang => bagus\n",
      "negation word detected :  tidak => alas\n",
      "negation word detected :  tidak => adil\n",
      "negation word detected :  kurang => sebentar\n",
      "negation word detected :  tidak => covidindonesia\n",
      "negation word detected :  belum => faham\n",
      "negation word detected :  tidak => bebas\n",
      "negation word detected :  tidak => sholat\n",
      "negation word detected :  tak => takut\n",
      "negation word detected :  tidak => selesaiindonesiaterserah\n",
      "negation word detected :  kurang => dlm\n",
      "negation word detected :  tidak => puas\n",
      "negation word detected :  tidak => layak\n",
      "negation word detected :  belum => angka\n",
      "negation word detected :  kurang => guna\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tidak => sasar\n",
      "negation word detected :  tidak => kumpulkumpul\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  belum => covid\n",
      "negation word detected :  tidak => mana\n",
      "negation word detected :  tak => covid\n",
      "negation word detected :  tidak => turun\n",
      "negation word detected :  tidak => tengok\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  tak => senyum\n",
      "negation word detected :  tak => salam\n",
      "negation word detected :  tidak => ya\n",
      "negation word detected :  tak => sama\n",
      "negation word detected :  tidak => sayang\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  tidak => mahal\n",
      "negation word detected :  tidak => ditundaindonesiaterserah\n",
      "negation word detected :  tidak => sedia\n",
      "negation word detected :  tak => apaapa\n",
      "negation word detected :  tak => ilmu\n",
      "negation word detected :  tidak => softskill\n",
      "negation word detected :  tak => hebat\n",
      "negation word detected :  tak => tunggu\n",
      "negation word detected :  tidak => muncul\n",
      "negation word detected :  tidak => jangkau\n",
      "negation word detected :  tidak => relevan\n",
      "negation word detected :  tidak => rapid\n",
      "negation word detected :  tak => klaim\n",
      "negation word detected :  tidak => urus\n",
      "negation word detected :  tidak => serah\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tidak => setia\n",
      "negation word detected :  tidak => singgung\n",
      "negation word detected :  tak => jua\n",
      "negation word detected :  tak => arah\n",
      "negation word detected :  tidak => test\n",
      "negation word detected :  tidak => jasa\n",
      "negation word detected :  belum => putus\n",
      "negation word detected :  tak => jaga\n",
      "negation word detected :  tak => masker\n",
      "negation word detected :  tak => diam\n",
      "negation word detected :  tidak => terap\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tidak => daftar\n",
      "negation word detected :  tidak => terima\n",
      "negation word detected :  tidak => terima\n",
      "negation word detected :  kurang => sdm\n",
      "negation word detected :  kurang => sdm\n",
      "negation word detected :  kurang => maksimal\n",
      "negation word detected :  tak => sudah\n",
      "negation word detected :  tak => habishabisnya\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  belum => kendali\n",
      "negation word detected :  tidak => mudah\n",
      "negation word detected :  tidak => mis\n",
      "negation word detected :  tidak => nyambung\n",
      "negation word detected :  tak => bayang\n",
      "negation word detected :  tidak => kena\n",
      "negation word detected :  tidak => engga\n",
      "negation word detected :  tak => percaya\n",
      "negation word detected :  tidak => yg\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  belum => terima\n",
      "negation word detected :  belum => bansos\n",
      "negation word detected :  tidak => ajar\n",
      "negation word detected :  belum => bukti\n",
      "negation word detected :  belum => aplikasi\n",
      "negation word detected :  tidak => tenang\n",
      "negation word detected :  tidak => masuk\n",
      "negation word detected :  tidak => kunjung\n",
      "negation word detected :  tidak => rumah\n",
      "negation word detected :  tidak => pimpin\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  tak => manfaat\n",
      "negation word detected :  tidak => taat\n",
      "negation word detected :  kurang => sebar\n",
      "negation word detected :  belum => terap\n",
      "negation word detected :  tidak => paham\n",
      "negation word detected :  tidak => main\n",
      "negation word detected :  tidak => pesimis\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  kurang => asik\n",
      "negation word detected :  tidak => ya\n",
      "negation word detected :  belum => dkijakarta\n",
      "negation word detected :  tidak => guna\n",
      "negation word detected :  tidak => bosan\n",
      "negation word detected :  tidak => pikir\n",
      "negation word detected :  kurang => bagus\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  tidak => taat\n",
      "negation word detected :  tak => rata\n",
      "negation word detected :  tidak => layak\n",
      "negation word detected :  tidak => kerumun\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tak => lupa\n",
      "negation word detected :  tidak => jajah\n",
      "negation word detected :  tak => tinggal\n",
      "negation word detected :  kurang => bagus\n",
      "negation word detected :  tidak => get\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  belum => jg\n",
      "negation word detected :  belum => terima\n",
      "negation word detected :  tidak => bilang\n",
      "negation word detected :  kurang => tular\n",
      "negation word detected :  tidak => yg\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tidak => putus\n",
      "negation word detected :  tidak => ikatanadmin\n",
      "negation word detected :  tidak => alih\n",
      "negation word detected :  tak => kunjung\n",
      "negation word detected :  tidak => napas\n",
      "negation word detected :  tak => kurang\n",
      "negation word detected :  kurang => orang\n",
      "negation word detected :  kurang => orang\n",
      "negation word detected :  tidak => tangan\n",
      "negation word detected :  tak => lolos\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tak => paham\n",
      "negation word detected :  tidak => siasia\n",
      "negation word detected :  tak => politisasi\n",
      "negation word detected :  tidak => hilang\n",
      "negation word detected :  tidak => kasihan\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  tidak => hasil\n",
      "negation word detected :  tak => picu\n",
      "negation word detected :  belum => bijak\n",
      "negation word detected :  tidak => kasat\n",
      "negation word detected :  tak => percaya\n",
      "negation word detected :  tak => duli\n",
      "negation word detected :  tak => percaya\n",
      "negation word detected :  belum => new\n",
      "negation word detected :  tidak => erti\n",
      "negation word detected :  belum => tuntas\n",
      "negation word detected :  tidak => dihadang\n",
      "negation word detected :  tidak => menyalahgunakan\n",
      "negation word detected :  tidak => transparan\n",
      "negation word detected :  tidak => bantu\n",
      "negation word detected :  belum => beres\n",
      "negation word detected :  belum => kritik\n",
      "negation word detected :  belum => data\n",
      "negation word detected :  tidak => diupdate\n",
      "negation word detected :  tak => tapi\n",
      "negation word detected :  tidak => sengaja\n",
      "negation word detected :  tidak => modal\n",
      "negation word detected :  tidak => stock\n",
      "negation word detected :  tidak => sewa\n",
      "negation word detected :  tidak => jaga\n",
      "negation word detected :  tak => raya\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tak => daya\n",
      "negation word detected :  kurang => gantung\n",
      "negation word detected :  tak => tag\n",
      "negation word detected :  kurang => sasar\n",
      "negation word detected :  belum => serah\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  tak => serah\n",
      "negation word detected :  tidak => tumpah\n",
      "negation word detected :  tidak => salah\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tidak => hak\n",
      "negation word detected :  kurang => sebelumnyahappy\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tak => picu\n",
      "negation word detected :  tidak => sukaa\n",
      "total tweet with negation :  200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation word detected :  belum => terima\n",
      "negation word detected :  tidak => harga\n",
      "negation word detected :  tidak => pulang\n",
      "negation word detected :  tidak => mudik\n",
      "negation word detected :  tidak => diingginkan\n",
      "negation word detected :  belum => landas\n",
      "negation word detected :  belum => ambil\n",
      "negation word detected :  belum => ambil\n",
      "negation word detected :  tak => tanding\n",
      "negation word detected :  tidak => diberitahu\n",
      "negation word detected :  tidak => indonesiaterserahhanya\n",
      "negation word detected :  tak => sasar\n",
      "negation word detected :  tak => percaya\n",
      "negation word detected :  belum => kelar\n",
      "negation word detected :  tidak => kalah\n",
      "negation word detected :  tidak => makna\n",
      "negation word detected :  tidak => negri\n",
      "negation word detected :  tidak => baru\n",
      "negation word detected :  kurang => timbang\n",
      "negation word detected :  kurang => sasar\n",
      "negation word detected :  tidak => busuk\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  tidak => jujur\n",
      "negation word detected :  belum => lockdown\n",
      "negation word detected :  kurang => tidak\n",
      "negation word detected :  tidak => hashtag\n",
      "negation word detected :  tidak => serah\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  kurang => konflik\n",
      "negation word detected :  tak => lepas\n",
      "negation word detected :  tidak => pimpin\n",
      "negation word detected :  belum => ad\n",
      "negation word detected :  tidak => pake\n",
      "negation word detected :  tidak => papa\n",
      "negation word detected :  tak => hirau\n",
      "negation word detected :  tidak => curi\n",
      "negation word detected :  tidak => makan\n",
      "negation word detected :  tak => serah\n",
      "negation word detected :  tidak => terima\n",
      "negation word detected :  tidak => kumpul\n",
      "negation word detected :  tidak => kemall\n",
      "negation word detected :  tidak => bagus\n",
      "negation word detected :  tidak => data\n",
      "negation word detected :  belum => selesai\n",
      "negation word detected :  belum => lengah\n",
      "negation word detected :  tidak => orang\n",
      "negation word detected :  tidak => daftar\n",
      "negation word detected :  belum => kerja\n",
      "negation word detected :  kurang => rentan\n",
      "negation word detected :  kurang => masyarakat\n",
      "negation word detected :  tak => campur\n",
      "negation word detected :  tidak => sentuh\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => gubris\n",
      "negation word detected :  tak => salah\n",
      "negation word detected :  belum => baharu\n",
      "negation word detected :  tak => bawa\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  kurang => karyawan\n",
      "negation word detected :  tak => gejolak\n",
      "negation word detected :  tidak => satu\n",
      "negation word detected :  belum => realisasi\n",
      "negation word detected :  tidak => lihat\n",
      "negation word detected :  belum => orientasi\n",
      "negation word detected :  tidak => data\n",
      "negation word detected :  kurang => anggar\n",
      "negation word detected :  tak => salah\n",
      "negation word detected :  tidak => fokus\n",
      "negation word detected :  belum => relaksasi\n",
      "negation word detected :  belum => terima\n",
      "negation word detected :  kurang => piknik\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  tak => kenal\n",
      "negation word detected :  tidak => tara\n",
      "negation word detected :  belum => proses\n",
      "negation word detected :  tak => sabar\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tidak => tepat\n",
      "total tweet with negation :  78\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(dataset):\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    negation_word = []\n",
    "    negated_word = []\n",
    "    negation_word_list = [\"tidak\",\"tanpa\",\"tak\",\"belum\",\"kurang\"]\n",
    "    count = 0\n",
    "    for row in dataset:\n",
    "        #print(\"before : \",row['tweet'])\n",
    "        #convert sentence in lowercase\n",
    "        row['tweet'] = row['tweet'].lower()\n",
    "        row['tweet'] = re.sub('[^A-Za-z0-9 ]+','', row['tweet'])\n",
    "        # remove number\n",
    "        row['tweet'] = re.sub('[0-9]+', '', row['tweet'])\n",
    "        row['tweet'] = re.sub(r'\\d+', '', row['tweet'])\n",
    "        #Convert www.* or https?://* to URL\n",
    "        row['tweet'] = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',row['tweet'])\n",
    "        row['tweet'] = re.sub(r\"pictwittercom(\\w+)\", '',row['tweet'])\n",
    "        #remove hastag word\n",
    "        row['tweet'] = re.sub(r\"#(\\w+)\", '',row['tweet'])\n",
    "        #remove atuser words\n",
    "        row['tweet'] = row['tweet'].replace('atuser','')\n",
    "        \n",
    "       \n",
    "        #Tokenize\n",
    "        word_tokens = word_tokenize(row['tweet']) \n",
    "        #Stop word removal\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stopWords]\n",
    "        row['tweet'] = \" \".join(filtered_sentence)\n",
    "        #stemming\n",
    "        filtered = [] \n",
    "        for v in filtered_sentence:\n",
    "            if \"NOT\" not in v:\n",
    "                filtered.append(stemmer.stem(v))\n",
    "            if \"NOT\" in v:\n",
    "                filtered.append(v)\n",
    "        row['tweet'] = \" \".join(filtered)\n",
    "        #negation handling\n",
    "        word_tokens = word_tokenize(row['tweet'])\n",
    "        for x in range(len(word_tokens)):\n",
    "            if word_tokens[x] in negation_word_list:\n",
    "                if(x+1 < len(word_tokens)):\n",
    "                    print(\"negation word detected : \",word_tokens[x], \"=>\",word_tokens[x+1])\n",
    "                    sentence = word_tokens[x] + \" \" + word_tokens[x+1]\n",
    "                    sentence_1 = \"not\" + \"_\" + word_tokens[x+1]\n",
    "                    negation_word.append(sentence)\n",
    "                    negated_word.append(sentence_1)\n",
    "                    count = count + 1\n",
    "        for x in range(len(negation_word)):\n",
    "            if(negation_word[x] in row['tweet']):\n",
    "                row['tweet'] = row['tweet'].replace(negation_word[x],negated_word[x])\n",
    "    print(\"total tweet with negation : \",count)\n",
    "preprocessing(dataset_training)\n",
    "preprocessing(dataset_testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>emotion</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>intent</th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/07/2020 12:14</td>\n",
       "      <td>anger</td>\n",
       "      <td>['#bansos', '#pandemic', '#koransindo']</td>\n",
       "      <td>negative</td>\n",
       "      <td>politisasi bansos covid not_menang pilkada awa...</td>\n",
       "      <td>pung purwanto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29/05/2020 10:19</td>\n",
       "      <td>love</td>\n",
       "      <td>['#bansos', '#bantuansosial', '#dtks', '#kemen...</td>\n",
       "      <td>positive</td>\n",
       "      <td>bansos bantuansosial dtks kemensos sumedang ja...</td>\n",
       "      <td>ruang berita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24/05/2020 00:55</td>\n",
       "      <td></td>\n",
       "      <td>['#indonesiaterserah']</td>\n",
       "      <td>positive</td>\n",
       "      <td>team rumah aja indonesiaterserah</td>\n",
       "      <td>abdul latief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/05/2020 07:46</td>\n",
       "      <td></td>\n",
       "      <td>['#indonesiaterserah', '#covidiot']</td>\n",
       "      <td>positive</td>\n",
       "      <td>kemarin yg semangat buka akses zona hijau indo...</td>\n",
       "      <td>omgcorona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13/07/2020 23:38</td>\n",
       "      <td>anger</td>\n",
       "      <td>['#kemendagri', '#pilkada2020', '#bansos']</td>\n",
       "      <td>negative</td>\n",
       "      <td>tito ingat calon tahana not_manfaat bansos cor...</td>\n",
       "      <td>republik merdeka banten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date emotion  \\\n",
       "0  07/07/2020 12:14   anger   \n",
       "1  29/05/2020 10:19    love   \n",
       "2  24/05/2020 00:55           \n",
       "3  26/05/2020 07:46           \n",
       "4  13/07/2020 23:38   anger   \n",
       "\n",
       "                                             hashtag    intent  \\\n",
       "0            ['#bansos', '#pandemic', '#koransindo']  negative   \n",
       "1  ['#bansos', '#bantuansosial', '#dtks', '#kemen...  positive   \n",
       "2                             ['#indonesiaterserah']  positive   \n",
       "3                ['#indonesiaterserah', '#covidiot']  positive   \n",
       "4         ['#kemendagri', '#pilkada2020', '#bansos']  negative   \n",
       "\n",
       "                                               tweet                 username  \n",
       "0  politisasi bansos covid not_menang pilkada awa...            pung purwanto  \n",
       "1  bansos bantuansosial dtks kemensos sumedang ja...             ruang berita  \n",
       "2                   team rumah aja indonesiaterserah             abdul latief  \n",
       "3  kemarin yg semangat buka akses zona hijau indo...                omgcorona  \n",
       "4  tito ingat calon tahana not_manfaat bansos cor...  republik merdeka banten  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training = pd.DataFrame(dataset_training)\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>emotion</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>intent</th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/06/2020 16:39</td>\n",
       "      <td>sad</td>\n",
       "      <td>['#bansos']</td>\n",
       "      <td>negative</td>\n",
       "      <td>aku laku kasar tampar bansos bansos</td>\n",
       "      <td>jpnncom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13/06/2020 09:44</td>\n",
       "      <td>anger</td>\n",
       "      <td>['#indonesiaterserah']</td>\n",
       "      <td>negative</td>\n",
       "      <td>indonesia indonesiaterserah</td>\n",
       "      <td>maung pasar andir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/06/2020 00:03</td>\n",
       "      <td>anger</td>\n",
       "      <td>['#indonesiaterserah']</td>\n",
       "      <td>negative</td>\n",
       "      <td>miris negara taekland indonesiaterserah</td>\n",
       "      <td>elfahri100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07/07/2020 15:14</td>\n",
       "      <td>joy</td>\n",
       "      <td>['#disdukcapil', '#kpm', '#danadesa', '#bansos...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kpm not_terima bltdd tahap cepat urus disdukca...</td>\n",
       "      <td>lampungpostid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25/05/2020 21:47</td>\n",
       "      <td>joy</td>\n",
       "      <td>['#dirumahaja', '#covid19update', '#indonesiat...</td>\n",
       "      <td>positive</td>\n",
       "      <td>baju alhamdulillahmohon maaf lahir batindiruma...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date emotion  \\\n",
       "0  01/06/2020 16:39     sad   \n",
       "1  13/06/2020 09:44   anger   \n",
       "2  02/06/2020 00:03   anger   \n",
       "3  07/07/2020 15:14     joy   \n",
       "4  25/05/2020 21:47     joy   \n",
       "\n",
       "                                             hashtag    intent  \\\n",
       "0                                        ['#bansos']  negative   \n",
       "1                             ['#indonesiaterserah']  negative   \n",
       "2                             ['#indonesiaterserah']  negative   \n",
       "3  ['#disdukcapil', '#kpm', '#danadesa', '#bansos...  positive   \n",
       "4  ['#dirumahaja', '#covid19update', '#indonesiat...  positive   \n",
       "\n",
       "                                               tweet           username  \n",
       "0                aku laku kasar tampar bansos bansos            jpnncom  \n",
       "1                        indonesia indonesiaterserah  maung pasar andir  \n",
       "2            miris negara taekland indonesiaterserah         elfahri100  \n",
       "3  kpm not_terima bltdd tahap cepat urus disdukca...      lampungpostid  \n",
       "4  baju alhamdulillahmohon maaf lahir batindiruma...                     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing = pd.DataFrame(dataset_testing)\n",
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of positive tweets :  619\n",
      "Count of negative tweets :  592\n"
     ]
    }
   ],
   "source": [
    "filtered_data_negative = df_training[df_training[\"intent\"]=='negative']\n",
    "filtered_data_positive = df_training[df_training[\"intent\"]=='positive']\n",
    "\n",
    "\n",
    "print(\"Count of positive tweets : \", len(filtered_data_positive))\n",
    "print(\"Count of negative tweets : \", len(filtered_data_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of positive tweets :  248\n",
      "Count of negative tweets :  274\n"
     ]
    }
   ],
   "source": [
    "filtered_data_negative = df_testing[df_testing[\"intent\"]=='negative']\n",
    "filtered_data_positive = df_testing[df_testing[\"intent\"]=='positive']\n",
    "\n",
    "\n",
    "print(\"Count of positive tweets : \", len(filtered_data_positive))\n",
    "print(\"Count of negative tweets : \", len(filtered_data_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction TF IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'positive': 619, 'negative': 592})\n"
     ]
    }
   ],
   "source": [
    "X_train = df_training['tweet']\n",
    "y_train = df_training['intent']\n",
    "#TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4706\n"
     ]
    }
   ],
   "source": [
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dppawpi',\n",
       " 'dpr',\n",
       " 'dprd',\n",
       " 'dprddki',\n",
       " 'dprdpati',\n",
       " 'dprri',\n",
       " 'dpt',\n",
       " 'dr',\n",
       " 'drakor',\n",
       " 'draksaid',\n",
       " 'drastis',\n",
       " 'dri',\n",
       " 'dropped']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[1001:1014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of feature on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e88f18248f20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf_idf_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf_idf_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtf_idf_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_idf_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "tf_idf_score = dict(zip(vocab, X[0]))\n",
    "for row in tf_idf_score:\n",
    "    if tf_idf_score[row] > 0 :\n",
    "        print (row, \" : \", tf_idf_score[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build naive bayes classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "text_classifier = MultinomialNB(alpha = 1, fit_prior = True, class_prior = None)\n",
    "text_classifier.fit(X_train_tfidf, y_train)\n",
    "text_classifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_class_prob_sorted = text_classifier.feature_log_prob_\n",
    "pos_class_prob_sorted = text_classifier.feature_log_prob_\n",
    "\n",
    "neg_dict = dict(zip(vocab, neg_class_prob_sorted[0]))\n",
    "pos_dict = dict(zip(vocab, pos_class_prob_sorted[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log probability of prior class :  [-0.71569511 -0.67109647]\n",
      "number of features in the model :  4706\n"
     ]
    }
   ],
   "source": [
    "print(\"log probability of prior class : \", text_classifier.class_log_prior_)\n",
    "print(\"number of features in the model : \",text_classifier.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'negative': 274, 'positive': 248})\n",
      "522\n"
     ]
    }
   ],
   "source": [
    "X_test = df_testing['tweet']\n",
    "y_test = df_testing['intent']\n",
    "#TFIDF\n",
    "#tfidf_vectorizer.transform(X)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "print(Counter(y_test))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78       274\n",
      "    positive       0.75      0.81      0.78       248\n",
      "\n",
      "    accuracy                           0.78       522\n",
      "   macro avg       0.78      0.78      0.78       522\n",
      "weighted avg       0.78      0.78      0.78       522\n",
      "\n",
      "0.7816091954022989\n"
     ]
    }
   ],
   "source": [
    "predictions = text_classifier.predict(X_test_tfidf)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : [0.81481481 0.75619835 0.80165289 0.76859504 0.80578512]\n",
      "Precision : [0.80152672 0.77118644 0.77142857 0.75       0.79844961]\n",
      "Recall : [0.84677419 0.7398374  0.87096774 0.82258065 0.83064516]\n",
      "F1 : [0.82352941 0.75518672 0.81818182 0.78461538 0.81422925]\n",
      "average accuracy :  0.7894092439546985\n",
      "average precision :  0.778518268413378\n",
      "average recall :  0.8221610280618936\n",
      "average f1 :  0.7991485171130936\n"
     ]
    }
   ],
   "source": [
    "y = y.map({'positive': 1, 'negative': 0}).astype(int)\n",
    "results_accuracy = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"accuracy\")\n",
    "print(\"Accuracy :\", results_accuracy)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = np.array([number[0] for number in lb.fit_transform(y)])\n",
    "results_precision = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"precision\")\n",
    "print(\"Precision :\", results_precision)\n",
    "results_recall = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"recall\")\n",
    "print(\"Recall :\", results_recall)\n",
    "results_f1 = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"f1\")\n",
    "print(\"F1 :\", results_f1)\n",
    "\n",
    "total = 0\n",
    "for x in results_accuracy:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average accuracy : \",total/5)\n",
    "\n",
    "total = 0\n",
    "for x in results_precision:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average precision : \",total/5)\n",
    "\n",
    "total = 0\n",
    "for x in results_recall:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average recall : \",total/5)\n",
    "\n",
    "total = 0\n",
    "for x in results_f1:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average f1 : \",total/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos :  286\n",
      "neg :  236\n"
     ]
    }
   ],
   "source": [
    "count_neg = 0\n",
    "count_pos = 0\n",
    "for row in predictions:\n",
    "    if row == \"negative\":\n",
    "        count_neg = count_neg +1 \n",
    "    if row == \"positive\":\n",
    "        count_pos = count_pos +1\n",
    "        \n",
    "print(\"pos : \",count_pos)\n",
    "print(\"neg : \",count_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
