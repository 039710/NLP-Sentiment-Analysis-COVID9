{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import imblearn\n",
    "import sklearn.metrics as metrics\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from numpy import where\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening Dataset and convert it into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets :  1211\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open ('../dataset/dataset_training.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=';')\n",
    "    \n",
    "    for row in readCSV:\n",
    "        tweets = row['tweet']    \n",
    "        usernames = row['username']\n",
    "        hashtags = row['hashtag']\n",
    "        dates = row['date']\n",
    "        intent = row['intent']\n",
    "        emotion = row['emotion']\n",
    "        dataset.append({\n",
    "            'date' : dates,\n",
    "            'username' : usernames,\n",
    "            'tweet' : tweets,\n",
    "            'hashtag' : hashtags,\n",
    "            'intent' : intent,\n",
    "            'emotion' : emotion\n",
    "        })\n",
    "            \n",
    "        \n",
    "print(\"total tweets : \" , len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of normalization are to remove non Indonesian word such as slang words and abbreviations will be converted to standard Indonesian words. the list is created by iterating to each feature in the dataset and list them with their normal word (standar word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of typo words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total typo words :  169\n"
     ]
    }
   ],
   "source": [
    "typo_list = []\n",
    "with open ('../feature_list/typo.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['typo']    \n",
    "        standard_word = row['standard_form']\n",
    "        typo_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })  \n",
    "print(\"total typo words : \" , len(typo_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of abbreviations words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total abbreviation words :  362\n"
     ]
    }
   ],
   "source": [
    "abbreviation_list = []\n",
    "with open ('../feature_list/abbreviation.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['abbreviations']    \n",
    "        standard_word = row['non-abbreviations']\n",
    "        abbreviation_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })      \n",
    "print(\"total abbreviation words : \" , len(abbreviation_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total slang words :  58\n"
     ]
    }
   ],
   "source": [
    "slang_list = []\n",
    "with open ('../feature_list/slangwords.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['slang_word']    \n",
    "        standard_word = row['formal_word']\n",
    "        slang_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })     \n",
    "print(\"total slang words : \" , len(slang_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load combination list of slang words,abbreviation words, and typo words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word_list :  697\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "with open ('../feature_list/word_list.csv',encoding=\"latin1\") as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        word = row['word']    \n",
    "        standard_word = row['formal_word']\n",
    "        word_list.append({\n",
    "            'word' : word,\n",
    "            'standard_word' : standard_word\n",
    "        })    \n",
    "print(\"total word_list : \" , len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def languagePreprocessing(dataset):\n",
    "    for row in dataset:\n",
    "        word_tokens = word_tokenize(row['tweet']) \n",
    "        filtered_word = []\n",
    "        for x in range(len(word_tokens)):\n",
    "            for n in range(len(word_list)):\n",
    "                if word_tokens[x] == word_list[n]['word']:\n",
    "                    word_tokens[x] = word_list[n]['standard_word']\n",
    "                    filtered_word.append(word_tokens[x])\n",
    "            filtered_word.append(word_tokens[x])\n",
    "            filtered_word = list(dict.fromkeys(filtered_word))\n",
    "        row['tweet']= \" \".join(filtered_word)\n",
    "languagePreprocessing(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After each list for each category listed, then I manually combine them into one csv file with two columns which are \"word\" and standard word\".\n",
    "By iterating each words after tokenization, if the word match in the document, then the word will be changed based on standard word in csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming, Tokenization, Stopwords removal, remove unnecessary attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load indonesian stopwords list (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword\n",
    "stopWords = []\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('atuser')\n",
    "    stopWords.append('url')\n",
    "    stopWords.append('via')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "st = open('../feature_list/stopwordsID.txt', 'r')\n",
    "stopWords = getStopWordList('../feature_list/stopwordsID.txt')\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation word detected :  tidak => menang\n",
      "negation word detected :  tidak => manfaat\n",
      "negation word detected :  tidak => nya\n",
      "negation word detected :  belum => mata\n",
      "negation word detected :  tidak => harga\n",
      "negation word detected :  tidak => bertanggungjawab\n",
      "negation word detected :  tidak => sasar\n",
      "negation word detected :  tidak => jangkau\n",
      "negation word detected :  tidak => paham\n",
      "negation word detected :  tidak => salah\n",
      "negation word detected :  tidak => pake\n",
      "negation word detected :  tidak => kerja\n",
      "negation word detected :  tidak => hasil\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tak => sajaindonesiaterserah\n",
      "negation word detected :  tidak => tetangga\n",
      "negation word detected :  tidak => nambahmasih\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => kerumun\n",
      "negation word detected :  tak => luput\n",
      "negation word detected :  tak => istilah\n",
      "negation word detected :  tidak => gengsi\n",
      "negation word detected :  kurang => sana\n",
      "negation word detected :  tidak => full\n",
      "negation word detected :  kurang => polsekkubu\n",
      "negation word detected :  tidak => bal\n",
      "negation word detected :  tidak => ambil\n",
      "negation word detected :  tidak => layak\n",
      "negation word detected :  tidak => jaman\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => koreksi\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => duli\n",
      "negation word detected :  tidak => sayang\n",
      "negation word detected :  kurang => bagus\n",
      "negation word detected :  tidak => alas\n",
      "negation word detected :  tidak => banget\n",
      "negation word detected :  kurang => sebentar\n",
      "negation word detected :  tidak => covidindonesia\n",
      "negation word detected :  belum => faham\n",
      "negation word detected :  tidak => bebas\n",
      "negation word detected :  tidak => biaya\n",
      "negation word detected :  tidak => sholat\n",
      "negation word detected :  tak => takut\n",
      "negation word detected :  tidak => pake\n",
      "negation word detected :  tidak => liat\n",
      "negation word detected :  tidak => selesaiindonesiaterserah\n",
      "negation word detected :  kurang => laksana\n",
      "negation word detected :  tidak => puas\n",
      "negation word detected :  tidak => layak\n",
      "negation word detected :  tidak => repot\n",
      "negation word detected :  belum => angka\n",
      "negation word detected :  tidak => n\n",
      "negation word detected :  kurang => negara\n",
      "negation word detected :  tidak => dpt\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tidak => om\n",
      "negation word detected :  tidak => takut\n",
      "negation word detected :  tidak => kumpulkumpul\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  belum => covid\n",
      "negation word detected :  tidak => mana\n",
      "negation word detected :  tak => covid\n",
      "negation word detected :  tidak => turun\n",
      "negation word detected :  tidak => bayang\n",
      "negation word detected :  tidak => pulang\n",
      "negation word detected :  tidak => banget\n",
      "negation word detected :  tak => sampai\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  tidak => hati\n",
      "negation word detected :  tak => senyum\n",
      "negation word detected :  tidak => ngeyelaaan\n",
      "negation word detected :  tidak => ya\n",
      "negation word detected :  tak => sama\n",
      "negation word detected :  tidak => sayang\n",
      "negation word detected :  tidak => dipake\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  tidak => mahal\n",
      "negation word detected :  tidak => sedia\n",
      "negation word detected :  tak => ilmu\n",
      "negation word detected :  tidak => softskill\n",
      "negation word detected :  tak => tunggu\n",
      "negation word detected :  tidak => pandang\n",
      "negation word detected :  tidak => muncul\n",
      "negation word detected :  tidak => pake\n",
      "negation word detected :  tidak => jangkau\n",
      "negation word detected :  tidak => relevan\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  tidak => harga\n",
      "negation word detected :  tidak => rapid\n",
      "negation word detected :  tak => klaim\n",
      "negation word detected :  tidak => urus\n",
      "negation word detected :  tidak => bakda\n",
      "negation word detected :  tidak => serah\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tidak => setia\n",
      "negation word detected :  tidak => munafik\n",
      "negation word detected :  tidak => banyak\n",
      "negation word detected :  tidak => nyerah\n",
      "negation word detected :  tidak => singgung\n",
      "negation word detected :  tak => lemah\n",
      "negation word detected :  tidak => nambah\n",
      "negation word detected :  tidak => test\n",
      "negation word detected :  tidak => jasa\n",
      "negation word detected :  tidak => salah\n",
      "negation word detected :  belum => putus\n",
      "negation word detected :  tak => jaga\n",
      "negation word detected :  tidak => suka\n",
      "negation word detected :  tidak => terap\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tidak => daftar\n",
      "negation word detected :  tidak => anya\n",
      "negation word detected :  tidak => ngedadak\n",
      "negation word detected :  tidak => terima\n",
      "negation word detected :  tidak => kurang\n",
      "negation word detected :  kurang => sdm\n",
      "negation word detected :  kurang => maksimal\n",
      "negation word detected :  tidak => nekan\n",
      "negation word detected :  tidak => bangku\n",
      "negation word detected :  tidak => pikir\n",
      "negation word detected :  tidak => dengerin\n",
      "negation word detected :  tak => sudah\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  tidak => usa\n",
      "negation word detected :  belum => kendali\n",
      "negation word detected :  tidak => mudah\n",
      "negation word detected :  tidak => didik\n",
      "negation word detected :  tidak => update\n",
      "negation word detected :  tidak => mis\n",
      "negation word detected :  tidak => syukur\n",
      "negation word detected :  tidak => percaya\n",
      "negation word detected :  tak => bayang\n",
      "negation word detected :  tidak => proses\n",
      "negation word detected :  tidak => nama\n",
      "negation word detected :  tidak => engga\n",
      "negation word detected :  tidak => perintah\n",
      "negation word detected :  tak => percaya\n",
      "negation word detected :  tidak => sia\n",
      "negation word detected :  tidak => ngikutin\n",
      "negation word detected :  tidak => harap\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  belum => terima\n",
      "negation word detected :  tidak => ajar\n",
      "negation word detected :  belum => bukti\n",
      "negation word detected :  tidak => orang\n",
      "negation word detected :  belum => aplikasi\n",
      "negation word detected :  tidak => tenang\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => masuk\n",
      "negation word detected :  tidak => kalah\n",
      "negation word detected :  tidak => rumah\n",
      "negation word detected :  tidak => relevan\n",
      "negation word detected :  tidak => tapi\n",
      "negation word detected :  tidak => kitu\n",
      "negation word detected :  tidak => pimpin\n",
      "negation word detected :  tidak => gmna\n",
      "negation word detected :  tidak => paham\n",
      "negation word detected :  tidak => dapat\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  tak => manfaat\n",
      "negation word detected :  tidak => taat\n",
      "negation word detected :  kurang => sebar\n",
      "negation word detected :  tidak => duit\n",
      "negation word detected :  belum => terap\n",
      "negation word detected :  tidak => paham\n",
      "negation word detected :  tidak => main\n",
      "negation word detected :  belum => bantu\n",
      "negation word detected :  kurang => asik\n",
      "negation word detected :  tidak => kompak\n",
      "negation word detected :  tidak => ya\n",
      "negation word detected :  belum => dkijakarta\n",
      "negation word detected :  tidak => pakai\n",
      "negation word detected :  tidak => guna\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => bosan\n",
      "negation word detected :  tidak => ngerti\n",
      "negation word detected :  tidak => psbb\n",
      "negation word detected :  kurang => bagus\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tak => taat\n",
      "negation word detected :  tak => rata\n",
      "negation word detected :  tidak => layak\n",
      "negation word detected :  tidak => si\n",
      "negation word detected :  tidak => kerumun\n",
      "negation word detected :  tidak => warga\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tidak => mesti\n",
      "negation word detected :  tak => lupa\n",
      "negation word detected :  tidak => jajah\n",
      "negation word detected :  tak => tinggal\n",
      "negation word detected :  tidak => full\n",
      "negation word detected :  kurang => bagus\n",
      "negation word detected :  tidak => get\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  belum => turun\n",
      "negation word detected :  belum => terima\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tidak => salah\n",
      "negation word detected :  tidak => sih\n",
      "negation word detected :  tidak => bilang\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  tidak => takut\n",
      "negation word detected :  kurang => tular\n",
      "negation word detected :  tidak => pergi\n",
      "negation word detected :  tidak => sesuai\n",
      "negation word detected :  tidak => kritik\n",
      "negation word detected :  tidak => putus\n",
      "negation word detected :  tidak => tunda\n",
      "negation word detected :  tidak => ikatanadmin\n",
      "negation word detected :  tidak => larang\n",
      "negation word detected :  tidak => ajar\n",
      "negation word detected :  tidak => tapi\n",
      "negation word detected :  tidak => tinggal\n",
      "negation word detected :  tidak => alih\n",
      "negation word detected :  tak => kunjung\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation word detected :  tidak => napas\n",
      "negation word detected :  tak => kurang\n",
      "negation word detected :  kurang => orang\n",
      "negation word detected :  tidak => tangan\n",
      "negation word detected :  tak => lolos\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => singkron\n",
      "negation word detected :  tak => paham\n",
      "negation word detected :  tidak => siasia\n",
      "negation word detected :  tak => politisasi\n",
      "negation word detected :  tidak => bahaya\n",
      "negation word detected :  tidak => hilang\n",
      "negation word detected :  tidak => rakyat\n",
      "negation word detected :  tak => peduli\n",
      "negation word detected :  tidak => hasil\n",
      "negation word detected :  tak => picu\n",
      "negation word detected :  belum => bijak\n",
      "negation word detected :  tidak => kasat\n",
      "negation word detected :  tak => percaya\n",
      "negation word detected :  tidak => serah\n",
      "negation word detected :  tidak => sii\n",
      "negation word detected :  belum => new\n",
      "negation word detected :  tidak => erti\n",
      "negation word detected :  belum => tuntas\n",
      "negation word detected :  tidak => dihadang\n",
      "negation word detected :  tidak => dpt\n",
      "negation word detected :  tidak => tuju\n",
      "negation word detected :  tidak => menyalahgunakan\n",
      "negation word detected :  tidak => transparan\n",
      "negation word detected :  tidak => bantu\n",
      "negation word detected :  belum => beres\n",
      "negation word detected :  belum => data\n",
      "negation word detected :  tidak => diupdate\n",
      "negation word detected :  tak => tapi\n",
      "negation word detected :  tidak => mari\n",
      "negation word detected :  tidak => sengaja\n",
      "negation word detected :  tidak => menang\n",
      "negation word detected :  tidak => nyerah\n",
      "negation word detected :  tidak => modal\n",
      "negation word detected :  tidak => lak\n",
      "negation word detected :  tak => raya\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tidak => keras\n",
      "negation word detected :  tidak => nyambung\n",
      "negation word detected :  tidak => manfaatin\n",
      "negation word detected :  tidak => ramai\n",
      "negation word detected :  tak => daya\n",
      "negation word detected :  kurang => gantung\n",
      "negation word detected :  tidak => peduli\n",
      "negation word detected :  tak => tag\n",
      "negation word detected :  kurang => sasar\n",
      "negation word detected :  belum => serah\n",
      "negation word detected :  tidak => muncul\n",
      "negation word detected :  tidak => tau\n",
      "negation word detected :  tak => serah\n",
      "negation word detected :  tidak => tumpah\n",
      "negation word detected :  tidak => baca\n",
      "negation word detected :  tidak => salah\n",
      "negation word detected :  tidak => taat\n",
      "negation word detected :  tidak => hak\n",
      "negation word detected :  tidak => bal\n",
      "negation word detected :  kurang => sebelumnyahappy\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tak => picu\n",
      "negation word detected :  tidak => indonesiaterserah\n",
      "negation word detected :  tidak => turun\n",
      "negation word detected :  tidak => sukaa\n",
      "total tweet with negation :  283\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(dataset):\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    negation_word = []\n",
    "    negated_word = []\n",
    "    negation_word_list = [\"tidak\",\"tanpa\",\"tak\",\"belum\",\"kurang\"]\n",
    "    count = 0\n",
    "    for row in dataset:\n",
    "        #print(\"before : \",row['tweet'])\n",
    "        #convert sentence in lowercase\n",
    "        row['tweet'] = row['tweet'].lower()\n",
    "        row['tweet'] = re.sub('[^A-Za-z0-9 ]+','', row['tweet'])\n",
    "        # remove number\n",
    "        row['tweet'] = re.sub('[0-9]+', '', row['tweet'])\n",
    "        row['tweet'] = re.sub(r'\\d+', '', row['tweet'])\n",
    "        #Convert www.* or https?://* to URL\n",
    "        row['tweet'] = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',row['tweet'])\n",
    "        row['tweet'] = re.sub(r\"pictwittercom(\\w+)\", '',row['tweet'])\n",
    "        #remove hastag word\n",
    "        row['tweet'] = re.sub(r\"#(\\w+)\", '',row['tweet'])\n",
    "        #remove atuser words\n",
    "        row['tweet'] = row['tweet'].replace('atuser','')\n",
    "        \n",
    "       \n",
    "        #Tokenize\n",
    "        word_tokens = word_tokenize(row['tweet']) \n",
    "        #Stop word removal\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stopWords]\n",
    "        row['tweet'] = \" \".join(filtered_sentence)\n",
    "        #stemming\n",
    "        filtered = [] \n",
    "        for v in filtered_sentence:\n",
    "            if \"NOT\" not in v:\n",
    "                filtered.append(stemmer.stem(v))\n",
    "            if \"NOT\" in v:\n",
    "                filtered.append(v)\n",
    "        row['tweet'] = \" \".join(filtered)\n",
    "        #negation handling\n",
    "        word_tokens = word_tokenize(row['tweet'])\n",
    "        for x in range(len(word_tokens)):\n",
    "            if word_tokens[x] in negation_word_list:\n",
    "                if(x+1 < len(word_tokens)):\n",
    "                    print(\"negation word detected : \",word_tokens[x], \"=>\",word_tokens[x+1])\n",
    "                    sentence = word_tokens[x] + \" \" + word_tokens[x+1]\n",
    "                    sentence_1 = \"not\" + \"_\" + word_tokens[x+1]\n",
    "                    negation_word.append(sentence)\n",
    "                    negated_word.append(sentence_1)\n",
    "                    count = count + 1\n",
    "        for x in range(len(negation_word)):\n",
    "            if(negation_word[x] in row['tweet']):\n",
    "                row['tweet'] = row['tweet'].replace(negation_word[x],negated_word[x])\n",
    "    print(\"total tweet with negation : \",count)\n",
    "preprocessing(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>emotion</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>intent</th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/07/2020 12:14</td>\n",
       "      <td>anger</td>\n",
       "      <td>['#bansos', '#pandemic', '#koransindo']</td>\n",
       "      <td>negative</td>\n",
       "      <td>politisasi bansos covid not_menang pilkada awa...</td>\n",
       "      <td>pung purwanto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29/05/2020 10:19</td>\n",
       "      <td>love</td>\n",
       "      <td>['#bansos', '#bantuansosial', '#dtks', '#kemen...</td>\n",
       "      <td>positive</td>\n",
       "      <td>bansos bantuansosial dtks tri sosial sumedang ...</td>\n",
       "      <td>ruang berita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24/05/2020 00:55</td>\n",
       "      <td></td>\n",
       "      <td>['#indonesiaterserah']</td>\n",
       "      <td>positive</td>\n",
       "      <td>team rumah indonesiaterserah</td>\n",
       "      <td>abdul latief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/05/2020 07:46</td>\n",
       "      <td></td>\n",
       "      <td>['#indonesiaterserah', '#covidiot']</td>\n",
       "      <td>positive</td>\n",
       "      <td>kemarin semangat buka akses zona hijau indones...</td>\n",
       "      <td>omgcorona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13/07/2020 23:38</td>\n",
       "      <td>anger</td>\n",
       "      <td>['#kemendagri', '#pilkada2020', '#bansos']</td>\n",
       "      <td>negative</td>\n",
       "      <td>tito ingat calon tahana not_manfaat bansos cor...</td>\n",
       "      <td>republik merdeka banten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date emotion  \\\n",
       "0  07/07/2020 12:14   anger   \n",
       "1  29/05/2020 10:19    love   \n",
       "2  24/05/2020 00:55           \n",
       "3  26/05/2020 07:46           \n",
       "4  13/07/2020 23:38   anger   \n",
       "\n",
       "                                             hashtag    intent  \\\n",
       "0            ['#bansos', '#pandemic', '#koransindo']  negative   \n",
       "1  ['#bansos', '#bantuansosial', '#dtks', '#kemen...  positive   \n",
       "2                             ['#indonesiaterserah']  positive   \n",
       "3                ['#indonesiaterserah', '#covidiot']  positive   \n",
       "4         ['#kemendagri', '#pilkada2020', '#bansos']  negative   \n",
       "\n",
       "                                               tweet                 username  \n",
       "0  politisasi bansos covid not_menang pilkada awa...            pung purwanto  \n",
       "1  bansos bantuansosial dtks tri sosial sumedang ...             ruang berita  \n",
       "2                       team rumah indonesiaterserah             abdul latief  \n",
       "3  kemarin semangat buka akses zona hijau indones...                omgcorona  \n",
       "4  tito ingat calon tahana not_manfaat bansos cor...  republik merdeka banten  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   politisasi bansos covid not_menang pilkada awas ketat pandemic koransindo\n",
      "1   bansos bantuansosial dtks tri sosial sumedang jabar indonesia kementeriansosial news ruberid\n",
      "2   team rumah indonesiaterserah\n",
      "3   kemarin semangat buka akses zona hijau indonesiaterserah covidiot\n",
      "4   tito ingat calon tahana not_manfaat bansos corona gambar kemendagri pilkada\n"
     ]
    }
   ],
   "source": [
    "for n in range (5):\n",
    "    print(n, ' ',dataset[n]['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of positive tweets :  619\n",
      "Count of negative tweets :  592\n"
     ]
    }
   ],
   "source": [
    "filtered_data_negative = df[df[\"intent\"]=='negative']\n",
    "filtered_data_positive = df[df[\"intent\"]=='positive']\n",
    "\n",
    "\n",
    "print(\"Count of positive tweets : \", len(filtered_data_positive))\n",
    "print(\"Count of negative tweets : \", len(filtered_data_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset are imbalance since the amount of positive tweets is way lower than negative tweets\n",
    "Since the classification wih imbalance class can cause low score on evaluation matrix such as for recall, a re-sampling method is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction TF IDF without re-sampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'positive': 619, 'negative': 592})\n"
     ]
    }
   ],
   "source": [
    "X = df['tweet']\n",
    "y = df['intent']\n",
    "#TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "X = tfidf_vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4504\n"
     ]
    }
   ],
   "source": [
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emildardak',\n",
       " 'emildardakinstruksikan',\n",
       " 'empati',\n",
       " 'enak',\n",
       " 'enam',\n",
       " 'endonesia',\n",
       " 'endus',\n",
       " 'energi',\n",
       " 'enforcement',\n",
       " 'engap',\n",
       " 'enjoy',\n",
       " 'ente',\n",
       " 'entire']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[1001:1014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of feature on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awas  :  0.3411394270493228\n",
      "bansos  :  0.10694536758215459\n",
      "covid  :  0.13769735186041135\n",
      "ketat  :  0.3804146482266746\n",
      "koransindo  :  0.41968986940402636\n",
      "not_menang  :  0.3967153378077462\n",
      "pandemic  :  0.3411394270493228\n",
      "pilkada  :  0.31816489545304266\n",
      "politisasi  :  0.3967153378077462\n"
     ]
    }
   ],
   "source": [
    "tf_idf_score = dict(zip(vocab, X[0]))\n",
    "for row in tf_idf_score:\n",
    "    if tf_idf_score[row] > 0 :\n",
    "        print (row, \" : \", tf_idf_score[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'positive': 708, 'negative': 679})\n",
      "Counter({'negative': 188, 'positive': 159})\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "#                                                    test_size=0.8, \n",
    "#                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of split and training data are 80:20 which mean 20% of dataset will be use for test dataset and 80% will be used for training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 4504)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build naive bayes classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "text_classifier = MultinomialNB(alpha = 1, fit_prior = True, class_prior = None)\n",
    "text_classifier.fit(X, y)\n",
    "text_classifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_class_prob_sorted = text_classifier.feature_log_prob_\n",
    "pos_class_prob_sorted = text_classifier.feature_log_prob_\n",
    "\n",
    "neg_dict = dict(zip(vocab, neg_class_prob_sorted[0]))\n",
    "pos_dict = dict(zip(vocab, pos_class_prob_sorted[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log probability of prior class :  [-0.71569511 -0.67109647]\n",
      "number of features in the model :  4504\n"
     ]
    }
   ],
   "source": [
    "print(\"log probability of prior class : \", text_classifier.class_log_prior_)\n",
    "print(\"number of features in the model : \",text_classifier.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## opem tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.76      0.80       188\n",
      "    positive       0.75      0.85      0.79       159\n",
      "\n",
      "    accuracy                           0.80       347\n",
      "   macro avg       0.80      0.80      0.80       347\n",
      "weighted avg       0.81      0.80      0.80       347\n",
      "\n",
      "0.7982708933717579\n"
     ]
    }
   ],
   "source": [
    "predictions = text_classifier.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : [0.81268012 0.78386167 0.77521614 0.81556196 0.76589595]\n",
      "Precision : [0.80337079 0.7816092  0.74742268 0.80555556 0.76436782]\n",
      "Recall : [0.8265896  0.78612717 0.83333333 0.83333333 0.76878613]\n",
      "F1 : [0.81481481 0.78386167 0.78804348 0.81920904 0.76657061]\n",
      "average accuracy :  0.79064316769669\n",
      "average precision :  0.7804652067958067\n",
      "average recall :  0.8096339113680153\n",
      "average f1 :  0.7944999218561535\n"
     ]
    }
   ],
   "source": [
    "y = y.map({'positive': 1, 'negative': 0}).astype(int)\n",
    "results_accuracy = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"accuracy\")\n",
    "print(\"Accuracy :\", results_accuracy)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = np.array([number[0] for number in lb.fit_transform(y)])\n",
    "results_precision = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"precision\")\n",
    "print(\"Precision :\", results_precision)\n",
    "results_recall = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"recall\")\n",
    "print(\"Recall :\", results_recall)\n",
    "results_f1 = cross_val_score(MultinomialNB(), X, y, cv=5,scoring=\"f1\")\n",
    "print(\"F1 :\", results_f1)\n",
    "\n",
    "total = 0\n",
    "for x in results_accuracy:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average accuracy : \",total/5)\n",
    "\n",
    "total = 0\n",
    "for x in results_precision:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average precision : \",total/5)\n",
    "\n",
    "total = 0\n",
    "for x in results_recall:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average recall : \",total/5)\n",
    "\n",
    "total = 0\n",
    "for x in results_f1:\n",
    "    total = total + x\n",
    "\n",
    "print(\"average f1 : \",total/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos :  181\n",
      "neg :  166\n"
     ]
    }
   ],
   "source": [
    "count_neg = 0\n",
    "count_pos = 0\n",
    "for row in predictions:\n",
    "    if row == \"negative\":\n",
    "        count_neg = count_neg +1 \n",
    "    if row == \"positive\":\n",
    "        count_pos = count_pos +1\n",
    "        \n",
    "print(\"pos : \",count_pos)\n",
    "print(\"neg : \",count_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse transform to obtain tweet text on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = []\n",
    "for row in range(len(X_test)):\n",
    "    X_test_text.append(tfidf_vectorizer.inverse_transform(X_test[row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_test_text = np.array(X_test_text)\n",
    "X_test_text  = X_test_text.ravel()\n",
    "X_test_text = np.array(X_test_text)\n",
    "print(type(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tweet = []\n",
    "for tweet in range(len(X_test_text)):\n",
    "    makeitastring = ' '.join(map(str, X_test_text[tweet]))\n",
    "    X_test_tweet.append(makeitastring)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get false positive and false negative on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tweet = []\n",
    "for tweet in range(len(X_test_text)):\n",
    "    makeitastring = ' '.join(map(str, X_test_text[tweet]))\n",
    "    X_test_tweet.append(makeitastring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True label</th>\n",
       "      <th>Predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>barudirumah barulembaran barusemangat covidnew...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>awas bansos bansosbebaskorupsi bio covid dukun...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>bulan imbas indonesiaterserah kawan keluarga k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>bansos cepat evaluasi juta kartu keluarga pagu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>aparat eh goblok indonesiaterserah kerumun mcd...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text True label  \\\n",
       "714  barudirumah barulembaran barusemangat covidnew...   positive   \n",
       "10   awas bansos bansosbebaskorupsi bio covid dukun...   negative   \n",
       "332  bulan imbas indonesiaterserah kawan keluarga k...   negative   \n",
       "118  bansos cepat evaluasi juta kartu keluarga pagu...   negative   \n",
       "224  aparat eh goblok indonesiaterserah kerumun mcd...   negative   \n",
       "\n",
       "    Predicted label  \n",
       "714        positive  \n",
       "10         negative  \n",
       "332        negative  \n",
       "118        positive  \n",
       "224        negative  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'Text':X_test_tweet,'True label':y_test, 'Predicted label' : predictions}\n",
    "df_test = pd.DataFrame(d)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_temp = []\n",
    "for row in y_test:\n",
    "    y_test_temp.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_fn = []\n",
    "true_label_fn = []\n",
    "predicted_label_fn = []\n",
    "predicted_label_fp = []\n",
    "tweet_text_fp = []\n",
    "true_label_fp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(y_test_temp)):\n",
    "    #false negative\n",
    "    if(y_test_temp[row] == \"positive\" and predictions[row] == \"negative\"):\n",
    "        #print(X_test_text[row], \" - \", y_test_temp[row], \" - \",predictions[row])\n",
    "        tweet_text_fn.append(X_test_text[row])\n",
    "        true_label_fn.append(y_test_temp[row])\n",
    "        predicted_label_fn.append(predictions[row])\n",
    "    #false positive\n",
    "    if(y_test_temp[row] == \"negative\" and predictions[row] == \"positive\"):\n",
    "        #print(X_test_text[row], \" - \", y_test_temp[row], \" - \",predictions[row])\n",
    "        tweet_text_fp.append(X_test_text[row])\n",
    "        true_label_fp.append(y_test_temp[row])\n",
    "        predicted_label_fp.append(predictions[row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_fn_ = []\n",
    "tweet_text_fp_ = []\n",
    "for x in tweet_text_fn:\n",
    "    makeitastring = ' '.join(map(str, x))\n",
    "    tweet_text_fn_.append(makeitastring)\n",
    "for x in tweet_text_fp:\n",
    "    makeitastring = ' '.join(map(str, x))\n",
    "    tweet_text_fp_.append(makeitastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>False neagtive tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bebas belanjakumparannewsindonesiaterserah bic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bansos duit giat hati matang rakyat rencana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adu bansos bantul corona posko tangan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ajasemoga indonesiaterserah ngedoain not_pedul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coba indonesia indonesiaterserah rubah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                False neagtive tweet\n",
       "0  bebas belanjakumparannewsindonesiaterserah bic...\n",
       "1        bansos duit giat hati matang rakyat rencana\n",
       "2              adu bansos bantul corona posko tangan\n",
       "3  ajasemoga indonesiaterserah ngedoain not_pedul...\n",
       "4             coba indonesia indonesiaterserah rubah"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'False neagtive tweet':tweet_text_fn_}\n",
    "df_fn = pd.DataFrame(d)\n",
    "df_fn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>False positive tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bansos cepat evaluasi juta kartu keluarga pagu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bansos banten berantas covid indikasi ketidakw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bansos bantu covid edyrahmayadi gubernur kota ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bansos bantu bantuanya beras bltlampung bupati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bansos beritaterkini birokrasi budi corona cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                False positive tweet\n",
       "0  bansos cepat evaluasi juta kartu keluarga pagu...\n",
       "1  bansos banten berantas covid indikasi ketidakw...\n",
       "2  bansos bantu covid edyrahmayadi gubernur kota ...\n",
       "3  bansos bantu bantuanya beras bltlampung bupati...\n",
       "4  bansos beritaterkini birokrasi budi corona cov..."
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'False positive tweet':tweet_text_fp_}\n",
    "df_fp = pd.DataFrame(d)\n",
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[129  34]\n",
      " [ 38 146]]\n",
      "Normalized confusion matrix\n",
      "[[0.79141104 0.20858896]\n",
      " [0.20652174 0.79347826]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(text_classifier, X_test, y_test,\n",
    "                                 display_labels=['Negative','Positive'],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custompreprocessing(tweet):\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    negation_word = []\n",
    "    negated_word = []\n",
    "    for x in range(len(word_tokens)):\n",
    "        if word_tokens[x] in negation_word_list:\n",
    "            if(x+1 < len(word_tokens)):\n",
    "                sentence = word_tokens[x] + \" \" + word_tokens[x+1]\n",
    "                sentence_1 = \"not\" + \"_\" + word_tokens[x+1]\n",
    "                negation_word.append(sentence)\n",
    "                negated_word.append(sentence_1)\n",
    "    for x in range(len(negation_word)):\n",
    "        if(negation_word[x] in tweet):\n",
    "            tweet = tweet.replace(negation_word[x],negated_word[x])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuspreprocessing(tweet):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    negation_word_list = [\"tidak\",\"tanpa\",\"tak\",\"belum\",\"kurang\"]\n",
    "    negation_word = []\n",
    "    negated_word = []\n",
    "    count = 0\n",
    "    for row in tweet:\n",
    "        #print(\"before : \",row['tweet'])\n",
    "        #convert sentence in lowercase\n",
    "        row = row.lower()\n",
    "        row = re.sub('[^A-Za-z0-9 ]+','', row)\n",
    "        # remove number\n",
    "        row = re.sub('[0-9]+', '', row)\n",
    "        row = re.sub(r'\\d+', '', row)\n",
    "        #Convert www.* or https?://* to URL\n",
    "        row = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',row)\n",
    "        row = re.sub(r\"pictwittercom(\\w+)\", '',row)\n",
    "        #remove hastag word\n",
    "        row = re.sub(r\"#(\\w+)\", '',row)\n",
    "        #remove atuser words\n",
    "        row = row.replace('atuser','')\n",
    "        \n",
    "        #Tokenize\n",
    "        word_tokens = word_tokenize(row)\n",
    "        #negation handling\n",
    "        for x in range(len(word_tokens)):\n",
    "            if word_tokens[x] in negation_word_list:\n",
    "                if(x+1 < len(word_tokens)):\n",
    "                    sentence = word_tokens[x] + \" \" + word_tokens[x+1]\n",
    "                    sentence_1 = \"not\" + \"_\" + word_tokens[x+1]\n",
    "                    negation_word.append(sentence)\n",
    "                    negated_word.append(sentence_1)\n",
    "                    print(negation_word, \" -> \",negated_word)\n",
    "                    count = count + 1\n",
    "        for x in range(len(negation_word)):\n",
    "            if(negation_word[x] in row):\n",
    "                row = row.replace(negation_word[x],negated_word[x])\n",
    "        word_tokens = word_tokenize(row)\n",
    "        #Stop word removal\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stopWords]\n",
    "        row = \" \".join(filtered_sentence)\n",
    "        \n",
    "        #stemming\n",
    "        filtered = [] \n",
    "        for v in filtered_sentence:\n",
    "            if \"not\" not in v:\n",
    "                filtered.append(stemmer.stem(v))\n",
    "            if \"not\" in v:\n",
    "                filtered.append(v)\n",
    "        row = \" \".join(filtered)\n",
    "        \n",
    "    print(\"total tweet with negation : \",count)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is created to detect negation in single tweet text and transform negated word into \"NOT_\"  + word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "neg_log_proba = []\n",
    "pos = []\n",
    "pos_log_proba = []\n",
    "for row in neg_dict:\n",
    "    if(\"not_\" in row):\n",
    "        neg.append(row)\n",
    "        neg_log_proba.append(neg_dict[row])\n",
    "\n",
    "for row in pos_dict:\n",
    "    if(\"not_\" in row):\n",
    "        pos.append(row)\n",
    "        pos_log_proba.append(pos_dict[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>negative log proba</th>\n",
       "      <th>positive log proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_acara</td>\n",
       "      <td>-8.968585</td>\n",
       "      <td>-8.724529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not_ajar</td>\n",
       "      <td>-8.393998</td>\n",
       "      <td>-8.975594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not_alas</td>\n",
       "      <td>-8.968585</td>\n",
       "      <td>-8.975594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_alih</td>\n",
       "      <td>-8.968585</td>\n",
       "      <td>-8.975594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_ambil</td>\n",
       "      <td>-8.968585</td>\n",
       "      <td>-8.669608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  negative log proba  positive log proba\n",
       "0  not_acara           -8.968585           -8.724529\n",
       "1   not_ajar           -8.393998           -8.975594\n",
       "2   not_alas           -8.968585           -8.975594\n",
       "3   not_alih           -8.968585           -8.975594\n",
       "4  not_ambil           -8.968585           -8.669608"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'feature':neg, \"negative log proba\":neg_log_proba,\"positive log proba\":pos_log_proba}\n",
    "df_log_proba = pd.DataFrame(d)\n",
    "df_log_proba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above shows the log probability of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log probability of prior class :  [-0.67812006 -0.70840356]\n"
     ]
    }
   ],
   "source": [
    "print(\"log probability of prior class : \", text_classifier.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'negation_word_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-dd131ee2f033>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"tidak ambil\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_test_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustompreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_test_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlog_proba_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_test_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_test_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-205-5f2a765a88aa>\u001b[0m in \u001b[0;36mcustompreprocessing\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnegated_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnegation_word_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'negation_word_list' is not defined"
     ]
    }
   ],
   "source": [
    "test_text = [\"tidak ambil\"]\n",
    "new_test_text = custompreprocessing(test_text[0])\n",
    "predict = text_classifier.predict(tfidf_vectorizer.transform([new_test_text]))\n",
    "log_proba_predict = text_classifier.predict_proba(tfidf_vectorizer.transform([new_test_text]))\n",
    "print(new_test_text)\n",
    "print(\"classified as : \",predict)\n",
    "print(\"log probability :\",log_proba_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\"tidak adaaaaaaa\"]\n",
    "new_test_text = custompreprocessing(test_text[0])\n",
    "predict = text_classifier.predict(tfidf_vectorizer.transform([new_test_text]))\n",
    "log_proba_predict = text_classifier.predict_proba(tfidf_vectorizer.transform([new_test_text]))\n",
    "print(new_test_text)\n",
    "print(\"classified as : \",predict)\n",
    "print(\"log probability :\",log_proba_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification of text \"tidak ambil\" is based on calculation of log probability of feautre.\n",
    "Since the feature of \"tidak ambil\" is more higher in positive log proba column, the model will classified this text as positive.\n",
    "\n",
    "If the feature is not listed on feature list the model will classified the feature as positive. The classification model classified the text of \"tidak adaaaaaaa\" because there is no feature of \"not_adaaaaa\" in the feature list. Since the log prior class of positive higher than negative class, this cause the model classified a document which has no feature in the model as positive.\n",
    "\n",
    "Since this model is created only from 1800 tweets and only 380 tweets contain negation word, this model still need to be enhancted by adding more negation data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\"tidak untuk saat ini tapi , saya senang \"]\n",
    "test_text_after = cuspreprocessing(test_text)\n",
    "predict = text_classifier.predict(tfidf_vectorizer.transform([test_text_after]))\n",
    "print(test_text_after)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\"saya senang, tapi tidak untuk saat ini \"]\n",
    "test_text_after = cuspreprocessing(test_text)\n",
    "predict = text_classifier.predict(tfidf_vectorizer.transform([test_text_after]))\n",
    "print(test_text_after)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the test text on first cell with text \"tidak untuk saat ini tapi, saya senang\" (not for now, but i am happy) is classified as negative, however if we evaluate the text, it should be classified as positive.\n",
    "naive bayes ignore word order for text classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in pos_dict:\n",
    "    print(word, pos_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for postive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ''\n",
    "for row in dataset:\n",
    "    tweet  = tweet + row['tweet'] + \" \"\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,background_color='white').generate(tweet)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud positive tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet = ''\n",
    "for row in dataset:\n",
    "    if row['intent'] == 'positive':\n",
    "        positive_tweet  = positive_tweet + row['tweet'] + \" \"\n",
    "positive_tweet = positive_tweet.replace('atuser ', '')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,background_color='white').generate(positive_tweet)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud negative tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweet = ''\n",
    "for row in dataset:\n",
    "    if row['intent'] == 'negative':\n",
    "        negative_tweet  = negative_tweet + row['tweet'] + \" \"\n",
    "negative_tweet = negative_tweet.replace('at_user ', '')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,background_color='white').generate(negative_tweet)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtagPreprocessing(dataset):\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    count = 0\n",
    "    for row in dataset:\n",
    "        #print(\"before : \",row['tweet'])\n",
    "        #convert sentence in lowercase\n",
    "        row['hashtag'] = row['hashtag'].lower()\n",
    "        row['tweet'] = re.sub('[^A-Za-z0-9 ]+','', row['tweet'])\n",
    "        #remove bracket\n",
    "        row['hashtag'] = row['hashtag'].replace('[','')\n",
    "        row['hashtag'] = row['hashtag'].replace(']','')\n",
    "        #remove #\n",
    "        row['hashtag'] = row['hashtag'].replace('#','')\n",
    "        #remove '\n",
    "        row['hashtag'] = row['hashtag'].replace(\"'\",'')\n",
    "        row['hashtag'] = row['hashtag'].replace(\",\",' ')\n",
    "hashtagPreprocessing(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code above is to remove unnecessary symbols such as ', #, [ and ] on hastag's column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = []\n",
    "for row in dataset:\n",
    "    word_tokens = word_tokenize(row['hashtag']) \n",
    "    for x in range(len(word_tokens)):\n",
    "        hashtag.append(word_tokens[x])\n",
    "print(Counter(hashtag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize each hastagh and store in array hastag\n",
    "##### by runnning this commented line \"#print(Counter(hashtag))\", we can get the frequencies of hastags used on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "Data = {'hashtag': [],\n",
    "        'count': []\n",
    "       }\n",
    "dfHashtag = DataFrame(Data,columns=['hashtag','count'])\n",
    "dfHashtag = dfHashtag.append(pd.DataFrame([[\"indonesiaterserah\",994],[\"bansos\",704],[\"covid\",205],[\"kemensoshadir\",101],[\"juliaribatubara\",97],\n",
    "                                          [\"covid\",83],[\"dirumahaja\",82],[\"indonesiaabnormal\",69],[\"newnormal\",66],[\"indonesiamelawancovid19\",45],\n",
    "                                          ],columns=dfHashtag.columns),ignore_index=True)\n",
    "dfHashtag = dfHashtag.astype({'hashtag': 'str', 'count': 'int'})\n",
    "dfHashtag\n",
    "# a simple line plot\n",
    "dfHashtag.plot(kind='bar',x='hashtag',y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet = []\n",
    "for row in dataset:\n",
    "    if row['intent'] == 'positive':\n",
    "        word_tokens = word_tokenize(row['tweet']) \n",
    "        for x in range(len(word_tokens)):\n",
    "            #check if word used in hashtag\n",
    "            #if word_tokens[x] not in hashtag:\n",
    "            positive_tweet.append(word_tokens[x])\n",
    "#print(Counter(positive_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {'word': [],\n",
    "        'count': []\n",
    "       }\n",
    "dfPositive = DataFrame(Data,columns=['word','count'])\n",
    "dfPositive = dfPositive.append(pd.DataFrame([[\"bansos\",478],[\"indonesiaterserah\",271],[\"covid\",219],[\"bantu\",190],[\"salur\",129],[\"mentri\",113],\n",
    "                                          [\"kemensoshadir\",94],[\"juliaribatubara\",91],[\"indonesia\",86],[\"terima\",86]\n",
    "                                          ],columns=dfPositive.columns),ignore_index=True)\n",
    "dfPositive = dfPositive.astype({'word': 'str', 'count': 'int'})\n",
    "dfPositive\n",
    "# a simple line plot\n",
    "dfPositive.plot(kind='bar',x='word',y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweet = []\n",
    "for row in dataset:\n",
    "    if row['intent'] == 'negative':\n",
    "        word_tokens = word_tokenize(row['tweet']) \n",
    "        for x in range(len(word_tokens)):\n",
    "            #check if word used in hashtag\n",
    "            #if word_tokens[x] not in hashtag:\n",
    "            negative_tweet.append(word_tokens[x])\n",
    "#print(Counter(negative_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {'word': [],\n",
    "        'count': []\n",
    "       }\n",
    "dfNegative = DataFrame(Data,columns=['word','count'])\n",
    "dfNegative = dfNegative.append(pd.DataFrame([[\"indonesiaterserah\",577],[\"bansos\",224],[\"covid\",221],[\"indonesia\",92],[\"pemerintah\",72],[\"orang\",67],\n",
    "                                          [\"corona\",60],[\"normal\",60],[\"udah\",57],[\"indonesiaabnormal\",55]\n",
    "                                          ],columns=dfNegative.columns),ignore_index=True)\n",
    "dfNegative = dfNegative.astype({'word': 'str', 'count': 'int'})\n",
    "dfNegative\n",
    "# a simple line plot\n",
    "dfNegative.plot(kind='bar',x='word',y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
